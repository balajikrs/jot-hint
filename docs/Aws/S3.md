# Simple Storage Service (S3)

- Object storage
- Global storage platform - regional based/resilient (data is stored in a specific aws region at rest)
  :::tip Imp
  Public service - runs in Aws public zone - unlimited - multiuser -
  can tolerate a failure of a AZ and has few replication abilities
  :::
- Economical and accessed by
  - HTTP
  - API
  - GUI
  - CLI
- Delivers Objects and buckets

# Objects and buckets

- object has a key - that Obj can be referenced using a key in a bucket
- by default none has access to a public bucket except the root user
- Value - content being stored / binary data of the images or videos
- object can be between 0 byte and 5 terabytes
- object has
  - version id
  - metadata
  - Access control
  - subresouces
- Buckets
  - created in specific aws region - data never leaves a region unless we do so
  - name should be globally unique
  - S3 has stable data sovereignty
  - Blast radius is a region
  - Buckets can have unlimited number of objects
  - Flat structure - all objects are stored at same level
  - 3 - 63 characters - all lowercase no underscores
  - starts with lowercase or number
  - cannot be formated as ip address like 1.1.1.1
  - soft limit 100 buckets for an account ; 1000 is hard limit per account
  - key is the name value is the data
  - folder like stuctures are just prefixes
  - cannot mount in linux or windows as they are not file or block storage
  - 1 EBS can be attached to one instance but s3 we can do multi access
  - great for offload in things
  - great for large scale data storage, distribution or upload
  - default for any input or output aws products

# S3 Encryption

- Client-Side Encryption
- SSE-C
- SSE-S3
- SSE-KMS
  - SSE-KMS impacts permissions and how it can achieve role separation.
- SSE-S3
- AES-256

# S3 Storage classes
## S3 Standard `should be accessed for frequently accessed data and non replaceable `
  - objects are replicated across minimum 3 AZ
  - 11 9's durablity means  for 10,000,000 objects 1 objects loss per 10000 years
  - Replication does MD5 checksums, Cyclic redundancy checks are used to detect and fix any data corruption
  - `when stored successfully/durably - s3 retuns http 1.1 200 ok status `
  - Billing
    - GB per month fee for data stored
    - $ per GB of data transfered `out`
    - IN is free
    - price per 1000 requests
    - No specific retrival fee
    - No Minumum duration
    - no Minimum size
  - Milli-second first byte latency
    - objects can be publicly available
## S3 Standard - IA ( Infrequent access - for Long lived data and where data access is infrequent and for tiny objects)
  - Cost effective compared to standard 
  - Comprise 
    - New cost component - `retrival fee` (per GB)
    - minimum usage charge of 30 days (even though you stored the data for 4 days it will be charged for 30 days)
    - even less data stored each obj is charge equivalent of 128 kb for per object
## S3 Standard - one zone - IA ( Infrequent access - )
  - Comprise 
    - New cost component - `retrival fee` (per GB)
    - minimum usage charge of 30 days (even though you stored the data for 4 days it will be charged for 30 days)
    - even less data stored each obj is charge equivalent of 128 kb for per object
    - stored in only one zone (no replication across multiple AZ)
  - Use for 
    - Long lived data 
    - non-critical / replaceable 
    - infrequent
    - Can be used for replica copies 
    - can be used for intermediate data we can loose
  - `Dont` use for
    - Only copy of data
    - critcal data
    - frequently accessed
    - temporary data or tiny data
# S3 Glacier - Instant access
  - data in chilled state
  - minimum duration 90 days of charge
  - Instant access
  - 
# S3 - Glacier - Flexible
  - data in cold state
  - 1/6 of the cost of s3-standard
  - Trade offs
    - They are for cold objects(they are not for warm or ready for use)
    - not immediately available
    - cannot be made public
    - Can see in s3 bucket but they are pointer to the object
    - retrival process / payment for retrival fee
    - when retriving they will be stored in a S3-IA bucket temporaryly and after accessing it will be removed
    - Retrival 3 options
      - expediated - 1-5 minutes
      - standard - 3-5 hours
      - bulk - 4-12 hours
      - Faster the retrival more the cost
    - First byte latency is minutes or hours
    - 40 kb minimum billable cost
    - 90 days minimum duration\
  - Used for 
    - archival data
    - frequent or real-time access is not needed
      - e.g. yearly access data
# S3 - Glacier Deep archive (`cheaper one`)
  - data in frozen state
  - Trade offs
    - 180 day minimum billable
    - 40 kb minimum billable size
    - no public access
    - retrival job needed
    - Retrival opions
      - standard - 12 hours
      - bulk - 48 hours
      - First byte retrival - hours/days
  - used for
    - Archival and rarely accesed
    - secondary long time retrival backups
    - legal and regulatory requiremnents
  - Dont use for
    - Backups (due to long retrival time)
# S3 - Inteligent Tiering
  - contains 5 different storage tiers
    - Frequent Access (like s3 std) 
    - Infrequent Access (s3 IA) 
    - Archive instant access (90 days)
    - Archive Access(90 - 270 days)
    - Deep Archive(180 - 730 days)
  - Monitoring cost - -per 1000 objects
  - Used for 
    - long lived data
    - changing or unknown patterns
# S3 Lifecycle configuration
  - to optimise cost over time
 - set of rules to s3 bucket
 - rules - can apply to whole bucket or groups(defined by prefix or type) of object in that bucket
  - Transition Actions
    - s3 std to IA after 30 days (eg )
  - Expiration actions
    - delete whatever object /object versions
  - Can't move objects based on acces frequency (only Inteligent tiering does this)
# S3 Replication
## Two types of replication
  - Cross Region Replication (CRR) - Source and destination buckets are in different region 
  - Same-region Replication (SRR) - Source and destination buckets are in same region
- Replication configuration is applied in source bucket
## Replication config has below
  - what destination bucket is
  - IAM role to use in the replication process [trust policy] permisions policy will contain the below
    - will have access to read the source bucket
    - replicate to destination bucket
  - replication is encrypted
  - If CRR - the destination bucket dont trust the roles of the source bucket since it is belong to different account. Hence the destination bucket needs a resource policy that role in the destination bucket can replicate.
### Replication options
  - All object in the bucket or subset of object(prefix/tag or combo filter to replicate)
  - Storage class to maintain - default is same class. or we can replicate to lower cost class like S3-IA / S3-1Z-IA
  - Ownership of the objects [ if SRR - it is owned by same account/If SRR - by default the object will be owned by source ; can be overridden]
  - Replication Time Control - RTC- 15 min replication sla - to monitor [can be used when we have concrete requirement that both buckets are insync]
#### Important to note for exam
- By default `no retroactive` replication - it will not replicate older objects and will start replicating only when turned on
- For replication of source bucket versioning should be turned on
- Batch replication can replicate older objects need to be configured
- By default not bi-directional replication - only one way; if configured bi-directional(recently added) is possible
- Unencrypted, SSE-s3 & SSE-KMS(with extra config), SSE-C (recently added)
- replication happens on objects that are belonging to source accounts only 
- No system events like lifecyle configuration will be replicated; ALso data in glacier/glacier deep archive will not be replicated
- By default delete markers are not replicated ; however that needs to be enabled
### usecase of replication and its types are
- SRR - Log Aggregation
- SRR - for PROD and TEST sync / between different functional teams
- SRR - Resilience due to sovereignity restrictions
- CRR - Global Resilience requirements
- CRR - Latency reduction
# Presigned URLs
- way to give access to one person or URL to give access to the resouce using our credentials in a safe and secured way
- Common use case [add diagram]
- requirement
  - Specify object
  - Expiry data date and time
```
  aws s3 presign <s3 URI> --expires-in <number of seconds>
```
 e.g. to generate a presigned URL for 3 minutes

 ```
  aws s3 presign s3://bucket/all.jpg --expires-in 180
```

#### Important to note for exam
- we can create url for objects that we dont have access to 
- when we use the url it uses the permissions that identity generated it
- Access denied means generating id never had access or it doesn't have access right now
- Don't generate with a role - URL stops working when temporary credentials expire. we should always use long term identity

# S3 Select & Glacier Select
- provides Ways to get part of object instead of full object that is present in s3
- s3 select let us to use SQL like statements
- Filters data in s3 and send to app thereby reducing cost of the data transfer and rejection
- 400% faster and 80% cheaper
- supports json, csv, parquet
- bzip2 compression for json & CSV

# S3 Events
- S3 event notification service
- Notifications generated when events happens on a bucket
- Notification can be delivered to SNS topics,SQS queues and Lambda functions
- Types (these to be setup in `Event Notification Config`)
  - Object `created` (put,post,copy,completeMultiPartUpload)- generating thumb nails when you upload a image
  - `Delete` (using *, delete, delete marker created) e.g. triggering security flows when object is delete
  - Object `restores` (post(initated)/ completed) e.g.move objects from s3 glacier or s3 glacier deep archive
  - `Replication` (OperationMissedThereshold,OperationReplicatedAfterThreshold,OperationNotTracked,OperationFailedReplication)
- Here s3 services automatically interact with SQS/SNS/Lambda/ as S3 principal. Here resource (lambda/ sqs/sns ) needs to be setup to allow access to s3 principal
- the only way to change lamda resource policy is through AWS CLI
- events are json; in lambda it will in event structure
- S3 events are old approach and supports certain AWS services
- `Events bridge` is the alternative and it supports more aws services and more events
- By default use `Events bridge`
# S3 Access Logs
- Server access logging provides detailed records for the `requests that are made to a bucket`. 
- Server access logs are useful for many applications. 
- For example, access log information can be useful in` security and access audits`. 
- It can also help you learn about your customer base and understand your Amazon S3 bill.
- Logs of one bucket will be stored in another bucket (target bucket)
- Logging is done by `s3 log delivery group`
## Enabling Logging
- CLI
- Console
- Put Bucket logging
- Enabling will take few hours to take effect
## Access
- `s3 log delivery group` should be given write access to target bucket using - setting up ACL Bucket rule
- these are log files
  - each contains records attributes
    - date and time
    - requestor
    - operation
    - status codes
    - error codes (simlar to apache log file)
  - Records are new line delimited
- single target buckets can be used for collecting multiple bucket logs and they can be separated by prefixes (this is controlled in the logging configuration in the source bucket)
- we need to personally handle the deletion / lifecycle of the logs and this is not built in the product
# S3 Object Lock
- 